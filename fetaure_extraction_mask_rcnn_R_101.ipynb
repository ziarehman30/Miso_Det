{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZZVCRUQSrNh",
    "outputId": "c93af232-d392-4d52-d156-f42454640126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/menthel/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2g_zw_ztAlfS"
   },
   "outputs": [],
   "source": [
    "def bordered_resize(img, scale, center=None, border_size=None, border_color=None):\n",
    "    if center == None:\n",
    "        center = (int(img.shape[1]), int(img.shape[0]))\n",
    "    if border_size == None:\n",
    "        border_size = (img.shape[1], img.shape[0])\n",
    "    if border_color == None:\n",
    "        border_color=(255, 255, 255)\n",
    "    M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "    return cv2.warpAffine(img, M, border_size, borderValue=border_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVsjiBwTSuXx"
   },
   "outputs": [],
   "source": [
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "\n",
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYBZPquKTIWG"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQY02_RbTLuG"
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbEKzKFgYsS8"
   },
   "outputs": [],
   "source": [
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "cfg = load_config_and_model_weights(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plgwmXhxYtfW",
    "outputId": "987bec8e-bd7c-43a8-ce8b-dcb19ee272c1"
   },
   "outputs": [],
   "source": [
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfE0BFLIY2uX"
   },
   "outputs": [],
   "source": [
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "\n",
    "    return images, batched_inputs\n",
    "\n",
    "#images, batched_inputs = prepare_image_inputs(cfg, img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49kCiONzY-Qp"
   },
   "outputs": [],
   "source": [
    "def get_features(model, images):\n",
    "    features = model.backbone(images.tensor)\n",
    "    return features\n",
    "\n",
    "#features = get_features(model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwl2p_OWZHhW"
   },
   "outputs": [],
   "source": [
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals\n",
    "\n",
    "#proposals = get_proposals(model, images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC-ZsPvPZLTT"
   },
   "outputs": [],
   "source": [
    "def get_box_features(model, features, proposals):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(1, 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list\n",
    "\n",
    "#box_features, features_list = get_box_features(model, features, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3xIKehgZUnG"
   },
   "outputs": [],
   "source": [
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "#pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWjLin64bAur"
   },
   "outputs": [],
   "source": [
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes\n",
    "\n",
    "#boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YAIs9b-bHUd"
   },
   "outputs": [],
   "source": [
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes\n",
    "\n",
    "#output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G23qHQhSBxIS"
   },
   "outputs": [],
   "source": [
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aujRzf3CN1R"
   },
   "outputs": [],
   "source": [
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes\n",
    "\n",
    "#keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzOABH6hCQf2"
   },
   "outputs": [],
   "source": [
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]\n",
    "\n",
    "#visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roPRhLjBCTHA",
    "outputId": "fde96130-6c08-43bf-8bfc-a97cbf93ccac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "# %cd /content/\n",
    "# user = input('User name: ')\n",
    "# password = getpass('Password: ')\n",
    "# password = urllib.parse.quote(password) # your password is converted into url format\n",
    "# cmd_string = f'git clone -b add_visualbert --single-branch https://{user}:{password}@github.com/gchhablani/transformers.git'\n",
    "# os.system(cmd_string)\n",
    "# cmd_string, password = \"\", \"\" # removing the password from the variable\n",
    "# %cd transformers\n",
    "# !pip install -e \".[dev]\"\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA1m7X0d-bGB"
   },
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file_name in os.listdir(\"path to image folder\"):\n",
    "  file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnPV0gPQ-j1D",
    "outputId": "4385638e-b00e-433a-c44d-7f5f19f5e5f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI3OT-wz5Cgl"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "import os\n",
    "for file_name in file_names:\n",
    "  img_list = []\n",
    "  name = \"img_\" + file_name\n",
    "  name = plt.imread(\"path to image folder\"+file_name)\n",
    "  #name = cv2.resize(name, (365, 252))\n",
    "  namef = \"img_\" + file_name + \"_bgr\"\n",
    "  namef = cv2.cvtColor(name, cv2.COLOR_RGB2BGR)\n",
    "  img_list.append(namef)\n",
    "  images, batched_inputs = prepare_image_inputs(cfg, img_list)\n",
    "  features = get_features(model, images)\n",
    "  proposals = get_proposals(model, images, features)\n",
    "  box_features, features_list = get_box_features(model, features, proposals)\n",
    "  pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)\n",
    "  boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)\n",
    "  output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "  temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "  keep_boxes, max_conf = [],[]\n",
    "  for keep_box, mx_conf in temp:\n",
    "    keep_boxes.append(keep_box)\n",
    "    max_conf.append(mx_conf)\n",
    "  MIN_BOXES=10\n",
    "  MAX_BOXES=100\n",
    "  keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "  visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "  np.save(\"path to save folder\"+file_name[:-4],visual_embeds[0].detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
